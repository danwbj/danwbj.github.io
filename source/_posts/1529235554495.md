---
title: nomad初体验
date: 2018-06-13 21:10:15
tags: [nomad,分布式，微服务]
category: [其他]
---

> Nomad是一个管理机器集群并在集群上运行应用程序的工具。支持多种驱动程序（Docker、VMS、Java）运行job,操作简单，多数据中心，可以跨数据中心调度。

### 安装
在nomad官网找到适合自己系统的安装包，这里以linux为列
首先下载二进制包并解压

```
# wget https://releases.hashicorp.com/nomad/0.8.4/nomad_0.8.4_linux_amd64.zip
unzip nomad_0.8.4_linux_amd64.zip
```

配置环境变量，使其全局生效,编辑系统变量文件 /etc/profile 在末尾增加如下配置：   

```
# export PATH=$PATH:/nomad_path/  （此处填写解压后的nomad文件地址）
```
使环境变量立即生效

```
# source /etc/profile
```

执行nomad命令，看到nomad提示，说明nomad安装成功

```
Usage: nomad [-version] [-help] [-autocomplete-(un)install] <command> [args]

Common commands:
    run         Run a new job or update an existing job
    stop        Stop a running job
    status      Display the status output for a resource
    alloc       Interact with allocations
    job         Interact with jobs
    node        Interact with nodes
    agent       Runs a Nomad agent

Other commands:
    acl             Interact with ACL policies and tokens
    agent-info      Display status information about the local agent
    deployment      Interact with deployments
    eval            Interact with evaluations
    namespace       Interact with namespaces
    operator        Provides cluster-level tools for Nomad operators
    quota           Interact with quotas
    sentinel        Interact with Sentinel policies
    server          Interact with servers
    ui              Open the Nomad Web UI
    version         Prints the Nomad version
```
### 启动Agent

为了简单，我们启动一个开发模式的agent，开发模式可以快速启动server端和client端   

```
# nomad agent -dev

==> Starting Nomad agent...
==> Nomad agent configuration:

                Client: true
             Log Level: DEBUG
                Region: global (DC: dc1)
                Server: true

==> Nomad agent started! Log data will stream in below:

    [INFO] serf: EventMemberJoin: nomad.global 127.0.0.1
    [INFO] nomad: starting 4 scheduling worker(s) for [service batch _core]
    [INFO] client: using alloc directory /tmp/NomadClient599911093
    [INFO] raft: Node at 127.0.0.1:4647 [Follower] entering Follower state
    [INFO] nomad: adding server nomad.global (Addr: 127.0.0.1:4647) (DC: dc1)
    [WARN] fingerprint.network: Ethtool not found, checking /sys/net speed file
    [WARN] raft: Heartbeat timeout reached, starting election
    [INFO] raft: Node at 127.0.0.1:4647 [Candidate] entering Candidate state
    [DEBUG] raft: Votes needed: 1
    [DEBUG] raft: Vote granted. Tally: 1
    [INFO] raft: Election won. Tally: 1
    [INFO] raft: Node at 127.0.0.1:4647 [Leader] entering Leader state
    [INFO] raft: Disabling EnableSingleNode (bootstrap)
    [DEBUG] raft: Node 127.0.0.1:4647 updated peer set (2): [127.0.0.1:4647]
    [INFO] nomad: cluster leadership acquired
    [DEBUG] client: applied fingerprints [arch cpu host memory storage network]
    [DEBUG] client: available drivers [docker exec java]
    [DEBUG] client: node registration complete
    [DEBUG] client: updated allocations at index 1 (0 allocs)
    [DEBUG] client: allocs: (added 0) (removed 0) (updated 0) (ignore 0)
    [DEBUG] client: state updated to ready
```
从输出可以看到我们已经成功启动了nomad server端和client端   
访问本地127.0.0.1:4646 可以看到Nomad自带的ui界面 

### 集群节点  
打开另一个终端，运行一下命令可以看到nomad集群上已经注册的节点

```
# nomad node status
ID        DC   Name                   Class   Drain  Eligibility  Status
53f694be  dc1  localhost.localdomain  <none>  false  eligible     ready
```

开发模式也启动了一个server端，所有运行下面的命令可以查看到server成员

```
# nomad server members
Name                          Address    Port  Status  Leader  Protocol  Build  Datacenter  Region
localhost.localdomain.global  127.0.0.1  4648  alive   true    2         0.8.1  dc1         global
```

如果需要停止agent只需要Ctrl+C，就可以停止一个agent

### Nomad Job

job是用户在使用Nomad时与之交互的主要配置，是Nomad应该运行的任务的声明性规范，Job有一个全局唯一的名称，有一个或者多个任务组。

#### 运行一个Job
进入到你的工作目录，使用nomad inti命令生成一个实例job配置文件:example.nomad

```
# cd /root/nomad/
# nomad init
Example job file written to example.nomad

```
运行job,可以看到job被分配到节点上运行   

```
# nomad job run example.nomad
==> Monitoring evaluation "301af722"
    Evaluation triggered by job "example"
    Evaluation within deployment: "56ea1af8"
    Allocation "05e3f77a" created: node "53f694be", group "cache"
    Evaluation status changed: "pending" -> "complete"
==> Evaluation "301af722" finished with status "complete"

```

查看Job状态

```
# nomad job status
ID       Type     Priority  Status   Submit Date
example  service  50        running  2018-06-13T19:28:21+08:00

```
 如果想查看一个Job的详细信息
 
```
# nomad server members
Name                          Address    Port  Status  Leader  Protocol  Build  Datacenter  Region
localhost.localdomain.global  127.0.0.1  4648  alive   true    2         0.8.1  dc1         global
[root@localhost nomad_cluster]# nomad job status example
ID            = example
Name          = example
Submit Date   = 2018-06-13T19:28:21+08:00
Type          = service
Priority      = 50
Datacenters   = dc1
Status        = running
Periodic      = false
Parameterized = false

Summary
Task Group  Queued  Starting  Running  Failed  Complete  Lost
cache       0       0         1        0       0         0

Latest Deployment
ID          = 56ea1af8
Status      = failed
Description = Failed due to unhealthy allocations

Deployed
Task Group  Desired  Placed  Healthy  Unhealthy
cache       1        1       0        1

Allocations
ID        Node ID   Task Group  Version  Desired  Status   Created    Modified
05e3f77a  53f694be  cache       0        run      running  3m51s ago  51s ago
```
#### 修改一个Job
现在我们编辑example.nomad文件中count的，设置为3,这个参数的作用是指定要运行的任务组数

```
# The "count" parameter specifies the number of the task groups that should
# be running under this group. This value must be non-negative and defaults
# to 1.
count = 3
```
运行nomad job 命令可以看到如果我们更新了job会发生什么变化

```
# nomad job plan example.nomad
+/- Job: "example"
+/- Task Group: "cache" (2 create, 1 in-place update)
  +/- Count: "1" => "3" (forces create)
      Task: "redis"

Scheduler dry-run:
- All tasks successfully allocated.

Job Modify Index: 9
To submit the job with version verification run:

nomad job run -check-index 9 example.nomad

When running the job with the check-index flag, the job will only be run if the
server side version matches the job modify index returned. If the index has
changed, another user has modified the job and the plan's results are
potentially invalid.
```
使用给出的更新命令去更新job

```
# nomad job run -check-index 9 example.nomad
==> Monitoring evaluation "153899d5"
    Evaluation triggered by job "example"
    Evaluation within deployment: "d1734e35"
    Allocation "fd9851d0" created: node "53f694be", group "cache"
    Allocation "05e3f77a" modified: node "53f694be", group "cache"
    Allocation "e6ca8a97" created: node "53f694be", group "cache"
    Evaluation status changed: "pending" -> "complete"
==> Evaluation "153899d5" finished with status "complete"
```
#### 停止job
停止job使用nomad stop 命令

```
# nomad stop example
==> Monitoring evaluation "ffc6fddd"
    Evaluation triggered by job "example"
    Evaluation within deployment: "d1734e35"
    Evaluation status changed: "pending" -> "complete"
==> Evaluation "ffc6fddd" finished with status "complete" 

```
停止后，使用nomad status 命令查看Job状态已经变为dead状态

```
[root@localhost nomad_cluster]# nomad job  status
ID       Type     Priority  Status          Submit Date
example  service  50        dead (stopped)  2018-06-13T19:49:41+08:00
```
### 搭建集群
现在开始我们搭建一个nomad集群，前边我们使用dev模式同时启动了一个nomad server以及client端，如果在生产环境，建议至少使用3-5台服务

#### 环境准备
我这里准备三台虚拟机

ip | 虚拟名
---|---
10.211.55.4 | s1
10.211.55.5 | s2
10.211.55.6 | s3

以下简称s1,s2,s3
#### 启动server
在s1虚机上工作目录创建服务器配置文件server.hcl，内容如下:

```
# touch server.hcl
# vi server.hcl

data_dir = "/tmp/server"

name = "server1"

server {
  enabled = true

  # Self-elect, should be 3 or 5 for production
  bootstrap_expect = 3
}

```
保存，退出，启动nomad server代理

```
# nomad agent -config server.hcl
==> Loaded configuration from server.hcl
==> Starting Nomad agent...
==> Nomad agent configuration:

                Client: false
             Log Level: INFO
                Region: global (DC: dc1)
                Server: true
               Version: 0.8.1

==> Nomad agent started! Log data will stream in below:

    2018/06/10 12:54:32 [INFO] raft: Initial configuration (index=0): []
    2018/06/10 12:54:32 [INFO] raft: Node at 10.211.55.4:4647 [Follower] entering Follower state (Leader: "")
    2018/06/10 12:54:32 [INFO] serf: EventMemberJoin: server1.global 10.211.55.4
    2018/06/10 12:54:32.125891 [INFO] nomad: starting 2 scheduling worker(s) for [service batch system _core]
    2018/06/10 12:54:32.126270 [INFO] nomad: adding server server1.global (Addr: 10.211.55.4:4647) (DC: dc1)
    2018/06/10 12:54:32.126983 [ERR] consul: error looking up Nomad servers: server.nomad: unable to query Consul datacenters: Get http://127.0.0.1:8500/v1/catalog/datacenters: dial tcp 127.0.0.1:8500: getsockopt: connection refused
    2018/06/10 12:54:33 [WARN] raft: no known peers, aborting election
    2018/06/10 12:54:42.238478 [ERR] worker: failed to dequeue evaluation: No cluster leader
    2018/06/10 12:54:42.647826 [ERR] worker: failed to dequeue evaluation: No cluster leader
    2018/06/10 12:54:47.530410 [ERR] worker: failed to dequeue evaluation: No cluster leader
    2018/06/10 12:54:47.877889 [ERR] worker: failed to dequeue evaluation: No cluster leader
```
这里可以看到agent 启动之后会去找leader，由于我们配置的bootstrap_expect参数为3，因此只有三个集群节点都启动之后，才回去选举一个leader   
接下来我们在s2, s3机器上也重复上面的操作，分别启动一个server,为了区分，在三个配置文件中我们使用 name区分，s1,s2,s3配置文件中的name分别为server1,server2,server

三台机器上的agent都启动成功之后，我们使用nomad join命令将三个server连接

```
在s2上执行
# nomad server join 10.211.55.4
在s3上执行
# nomad server join 10.211.55.5

```
这样就把三个server组成了一个集群,并且他们已经选举了一个leader

```
# nomad server members
Name            Address      Port  Status  Leader  Protocol  Build  Datacenter  Region
server1.global  10.211.55.4  4648  alive   false   2         0.8.1  dc1         global
server2.global  10.211.55.5  4648  alive   true    2         0.8.1  dc1         global
server3.global  10.211.55.6  4648  alive   false   2         0.8.1  dc1         global

```
#### 启动client
在s2虚机上工作目录创建服务器配置文件client1.hcl，内容如下:

```
# touch client1.hcl
# vi client1.hcl

log_level = "DEBUG"

data_dir = "/tmp/client1"

name = "client1"

client {
  enabled = true

  # For demo assume we are talking to server1. For production,
  # this should be like "nomad.service.consul:4647" and a system
  # like Consul used for service discovery.
  servers = ["10.211.55.5:4647"]
}

# Modify our port to avoid a collision with server1 and client1
ports {
  http = 5656
}

```
保存，退出，启动nomad client代理

```
[root@localhost nomad_cluster]# nomad agent -config client1.hcl
==> Loaded configuration from client1.hcl
==> Starting Nomad agent...
==> Nomad agent configuration:

                Client: true
             Log Level: DEBUG
                Region: global (DC: dc1)
                Server: false
               Version: 0.8.1

==> Nomad agent started! Log data will stream in below:

    2018/06/13 20:43:31.353729 [INFO] client: using state directory /tmp/client1/client
    2018/06/13 20:43:31.354585 [INFO] client: using alloc directory /tmp/client1/alloc
    2018/06/13 20:43:31.357777 [DEBUG] client.fingerprint_manager: built-in fingerprints: [arch cgroup consul cpu host memory network nomad signal storage vault env_aws env_gce]
    2018/06/13 20:43:35.446947 [DEBUG] driver.docker: image "nginx" (sha256:cd5239a0906a6ccf0562354852fae04bc5b52d72a2aff9a871ddb6bd57553569) reference count incremented: 1
    2018/06/13 20:43:35.452540 [DEBUG] client: starting task context for 'redis' (alloc '1bcb0f46-a710-769a-287e-7d432342f143')
    2018/06/13 20:43:35.453826 [DEBUG] client: starting task runners for alloc '1bcb0f46-a710-769a-287e-7d432342f143'
    2018/06/13 20:43:40.697028 [DEBUG] client: 3 evaluations triggered by node update
    2018/06/13 20:43:40.697068 [DEBUG] client: state updated to ready
    2018/06/13 20:43:40.869207 [DEBUG] client: state changed, updating node and re-registering.
    2018/06/13 20:43:40.878128 [INFO] client: node registration complete

```
重复上面的操作在s3上面也启动一个client端，使用一下命令查看节点

```
# nomad node-status
ID        DC   Name     Class   Drain  Eligibility  Status
85be1fd0  dc1  client1  <none>  false  eligible     ready
d3f05cb0  dc1  client2  <none>  false  eligible     ready
```
现在我们已经有了一个集群，我们在s1上提交一个job测试一下我们的集群，使用example.nomad，确保count=3

```
# nomad job run example.nomad
==> Monitoring evaluation "8095c3ec"
    Evaluation triggered by job "example"
    Allocation "7bc737e3" created: node "85be1fd0", group "cache"
    Allocation "22e53fa4" created: node "1a9b1e3d", group "cache"
    Allocation "2692e18b" created: node "1a9b1e3d", group "cache"
    Evaluation within deployment: "b44335b4"
    Allocation "7bc737e3" status changed: "pending" -> "running"
    Evaluation status changed: "pending" -> "complete"
==> Evaluation "8095c3ec" finished with status "complete"
```
输出中看到调度程序为其中一个客户机节点分配了两个任务，剩下的任务分配给第二个客户端。也可以使用nomad status命令查看

```
[root@centos-linux nomad_cluster]# nomad job status
ID             Type     Priority  Status          Submit Date
example        service  50        running         2018-06-13T20:57:26+08:00
example-api-3  service  50        dead (stopped)  2018-06-12T23:16:09+08:00
example-api-4  service  50        running         2018-06-12T23:18:55+08:00
example-api-5  service  50        dead (stopped)  2018-06-12T23:36:26+08:00
example-api-6  service  50        dead (stopped)  2018-06-13T00:55:06+08:00
nginx          service  50        dead (stopped)  2018-06-10T00:04:52+08:00
[root@centos-linux nomad_cluster]# nomad job status example
ID            = example
Name          = example
Submit Date   = 2018-06-13T20:57:26+08:00
Type          = service
Priority      = 50
Datacenters   = dc1
Status        = running
Periodic      = false
Parameterized = false

Summary
Task Group  Queued  Starting  Running  Failed  Complete  Lost
cache       0       0         3        0       3         0

Latest Deployment
ID          = b44335b4
Status      = failed
Description = Failed due to unhealthy allocations

Deployed
Task Group  Desired  Placed  Healthy  Unhealthy
cache       3        3       0        3

Allocations
ID        Node ID   Task Group  Version  Desired  Status    Created        Modified
22e53fa4  1a9b1e3d  cache       2        run      running   3d7h from now  3d7h from now
2692e18b  1a9b1e3d  cache       2        run      running   3d7h from now  3d7h from now
7bc737e3  85be1fd0  cache       2        run      running   3d7h from now  3d7h from now
```
到此，我们已经成功搭建了nomad集群，并在集群上运行了简单地Job!





















